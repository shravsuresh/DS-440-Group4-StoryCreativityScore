{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ3_3nfXZO0V"
   },
   "source": [
    "Example tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NFzTI46ZP_q",
    "outputId": "16b0c86f-5eb2-49c6-a0ec-4d86b8e9f3c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'cat', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /storage/home/sqs6406/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "sentence = \"This is a cat.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpAe6-4BavCC",
    "outputId": "f471fda7-b9c5-49b1-9c74-287c8c60c925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', \"'re\", 'you', 'doing', '?']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What're you doing?\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSHM50rUb7Wq",
    "outputId": "329b5a9f-61e7-4bc0-c7cf-74dc7fca41b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['State', 'College', 'is', 'a', 'home', 'rule', 'municipality', 'in', 'Centre', 'County', 'in', 'the', 'Commonwealth', 'of', 'Pennsylvania', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"State College is a home rule municipality in Centre County in the Commonwealth of Pennsylvania. \"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXJlL4ZneTbx"
   },
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t6gHePH_eaG0",
    "outputId": "7a7161fe-853c-4f93-c1c9-a9d42ec66880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'boy', \"'s\", 'cars', 'are', 'different', 'colors', '.']\n",
      "['The', 'boy', \"'s\", 'car', 'are', 'different', 'color', '.']\n",
      "['The', 'boy', \"'s\", 'cars', 'be', 'different', 'color', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /storage/home/sqs6406/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import these modules \n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "sentence = \"The boy's cars are different colors.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
    "print(lemmatized_tokens)\n",
    "\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(w, pos='v') for w in tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt2ZLCqthNYb"
   },
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qyo6zbX7hOpQ",
    "outputId": "2f0f3aa6-9c32-4740-e56c-9b2d328e571e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programer  :  program\n",
      "programing  :  program\n",
      "programers  :  program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize \n",
    "   \n",
    "ps = PorterStemmer() \n",
    "  \n",
    "# choose some words to be stemmed \n",
    "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "  \n",
    "for w in words: \n",
    "    print(w, \" : \", ps.stem(w)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rCRKfGFi-eO"
   },
   "source": [
    "Stemming v.s. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w5XuXeDXjEBM",
    "outputId": "69a91b82-22a3-4d1d-8719-e18c335c67a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:  ['studi', 'studi']\n",
      "Lemmatization:  ['study', 'study']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = ['studies', 'studying']\n",
    "\n",
    "stemmed_tokens = [ps.stem(w) for w in tokens]\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(w, pos='v') \n",
    "                      for w in tokens]\n",
    "\n",
    "print('Stemming: ', stemmed_tokens)\n",
    "print('Lemmatization: ', lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfWh18yLnqUI"
   },
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXogd5XzntwU",
    "outputId": "a3d4476a-a7fe-4970-b1f0-8366cb7ae835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[State College, Centre County, the Commonwealth of Pennsylvania]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(\"State College is a home rule municipality in Centre County in the Commonwealth of Pennsylvania.\")\n",
    "\n",
    "print([ent for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgFXyGWbLgSM"
   },
   "source": [
    "## Document Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKw-ug3BLqkn"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The data set weâ€™ll use is a list of over one million news headlines published over a period of 15 years and can be downloaded from [Kaggle](https://www.kaggle.com/therohk/million-headlines/data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Deo6R1tQtkY0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkConf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6b429396aa05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkConf' is not defined"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "sc=SparkContext(conf=conf)\n",
    "sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "v3aA5eBZM5lP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "                                               Story  index\n",
      "0  My lover has left. I miss him so much. I write...      0\n",
      "1  My mom was sending me a letter in the mail abo...      1\n",
      "2  I had to buy a stamp at the post office. I am ...      2\n",
      "3  I decided it was time for me to send my mom an...      3\n",
      "4  Rachel was sitting in her dorm and decided to ...      4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-37eea3f1087c>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_text['index'] = data_text.index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/storage/home/sqs6406/StoryResults.csv\", error_bad_lines=False);\n",
    "data_text = data[['Story']]\n",
    "data_text['index'] = data_text.index\n",
    "documents = data_text[:10]\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9QPGXQmNL11"
   },
   "source": [
    "## Data Pre-processing\n",
    "We will perform the following steps:\n",
    "1.   Tokenization: Split the text into sentences and the sentences into words. \n",
    "2.   Lowercase the words and remove punctuation.\n",
    "3.   Words that have fewer than 3 characters are removed.\n",
    "4.   All stopwords are removed.\n",
    "5.   Words are lemmatized â€” words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "6.   Words are stemmed â€” words are reduced to their root form.\n",
    "\n",
    "Loading gensim and nltk libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "jdc06xYHNkPB"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e1e882cd8de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimple_preprocess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPorterStemmer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer \n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(16)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMU8EmtTNsqi"
   },
   "source": [
    "Write a function to perform lemmatize and stem preprocessing steps on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JadBs0FJNuyk"
   },
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkfubiteN1yU"
   },
   "source": [
    "Select a document to preview after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ogxscHRiN2dh"
   },
   "outputs": [],
   "source": [
    "doc_sample = documents[documents['index'] == 3].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9dCESDBO5kh"
   },
   "source": [
    "Preprocess the headline text, saving the results as â€˜processed_docsâ€™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vqXBN0RSO8Cs"
   },
   "outputs": [],
   "source": [
    "processed_docs = documents['Story'].map(preprocess)\n",
    "print(processed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFemYz7YPiLd"
   },
   "source": [
    "##Bag of Words on the Data set\n",
    "\n",
    "Create a dictionary from â€˜processed_docsâ€™ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zj3sg0XoPktG"
   },
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "print('# of tokens: ', len(dictionary))\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4FMWAbDPzmF"
   },
   "source": [
    "###Gensim filter_extremes\n",
    "\n",
    "Filter out tokens that appear in\n",
    "*   less than 2 documents (absolute number) or\n",
    "*   more than 0.1 documents (fraction of total corpus size, not absolute number).\n",
    "*   after the above two steps, keep only the first 1000 most frequent tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8z48edM4QPSb"
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=2, no_above=0.5, keep_n=1000)\n",
    "print('# of tokens after filtering: ', len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9w-Yk20QghB"
   },
   "source": [
    "### Gensim doc2bow\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to â€˜bow_corpusâ€™, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtKdkEDlQivE"
   },
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYMSy-lgQ39W"
   },
   "source": [
    "Preview Bag Of Words for our sample preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vri8CyExQ4-1"
   },
   "outputs": [],
   "source": [
    "bow_doc_3 = bow_corpus[3]\n",
    "for i in range(len(bow_doc_3)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_3[i][0], \n",
    "                                               dictionary[bow_doc_3[i][0]], \n",
    "bow_doc_3[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UW9ntORERHOB"
   },
   "source": [
    "###TF-IDF\n",
    "\n",
    "Create tf-idf model object using models.TfidfModel on â€˜bow_corpusâ€™ and save it to â€˜tfidfâ€™, then apply transformation to the entire corpus and call it â€˜corpus_tfidfâ€™. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFZXa9MgRJth"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "\n",
    "\n",
    "score = 0\n",
    "\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    score += 1\n",
    "    #break\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "miEKPaQ1SCxo"
   },
   "source": [
    "Transform into sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJA_V1xMSFUp"
   },
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2csc\n",
    "corpus_tfidf_matrix = corpus2csc(corpus_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOLi0tKHTBRx"
   },
   "source": [
    "### Document clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfrNaUe9TFyC"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=7)\n",
    "clusters = model.fit_predict(corpus_tfidf_matrix.T)\n",
    "print(model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3lpjxRXu-gA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "SemDis_TF-IDF.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
